{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yousif-A2/Conv_AutoEncoder_Pytorch/blob/main/Part2_Yousif_Alnasser_Conv_AutoEncoder_Pytorch_CelebA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "jcVzrgVktMpo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import os\n",
        "import random\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "s69_rU0Lqbz5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "from torch.utils.data import sampler\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "8Dj9Vw6Bv31j"
      },
      "outputs": [],
      "source": [
        "\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'  # or ':16:8'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_eI-oLOqqEJ",
        "outputId": "8bd6635a-efd7-42f3-de67-75120c5f254c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "cuda_device_num = 0\n",
        "cuda_device = torch.device(f'cuda:{cuda_device_num}' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', cuda_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "xTPilPaPrzEs"
      },
      "outputs": [],
      "source": [
        "\n",
        "if torch.cuda.is_available():\n",
        "  # Useful for reproducibility as it ensures that the same algorithms are used across runs.\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  # Ensures that the cuDNN operations are deterministic and, therefore, the same input will produce the same output every time\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "# Ensures that operations in PyTorch will behave deterministically\n",
        "torch.use_deterministic_algorithms(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "v67E8rFJrZzZ"
      },
      "outputs": [],
      "source": [
        "my_seed = 101\n",
        "os.environ[\"PL_GLOBAL_SEED\"] = str(my_seed)\n",
        "random.seed(my_seed)\n",
        "np.random.seed(my_seed)\n",
        "torch.manual_seed(my_seed)\n",
        "torch.cuda.manual_seed_all(my_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYjFBgTq6LTx",
        "outputId": "f85a66d0-57d6-4512-9778-78d270533ca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/jessicali9530/celeba-dataset\n",
            "License(s): other\n",
            "celeba-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d jessicali9530/celeba-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "vFLJEGad6JkF"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('celeba-dataset.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('./data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "nCmktX5eLuC_"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split, DataLoader\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "# Set a fixed random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "dataset = datasets.ImageFolder(root='data/img_align_celeba', transform=transform)\n",
        "\n",
        "# Split sizes\n",
        "total_len = len(dataset)\n",
        "train_len = int(0.7 * total_len)\n",
        "val_len = int(0.15 * total_len)\n",
        "test_len = total_len - train_len - val_len  # 15%\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_len, val_len, test_len])\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "valid_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "BUjUfvChrvR5"
      },
      "outputs": [],
      "source": [
        "lr_rate = 0.0005\n",
        "batch_size = 32\n",
        "num_epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWJ01cpItqSL",
        "outputId": "b791667d-4f08-4045-b439-40717c80a193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set:\n",
            "\n",
            "Image batch dimensions: torch.Size([128, 3, 64, 64])\n",
            "Image label dimensions: torch.Size([128])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "Validation Set:\n",
            "Image batch dimensions: torch.Size([128, 3, 64, 64])\n",
            "Image label dimensions: torch.Size([128])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "\n",
            "Testing Set:\n",
            "Image batch dimensions: torch.Size([128, 3, 64, 64])\n",
            "Image label dimensions: torch.Size([128])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ],
      "source": [
        "# Dataset Sanity check 1:\n",
        "print('Training Set:\\n')\n",
        "for images, labels in train_loader:\n",
        "    print('Image batch dimensions:', images.size())\n",
        "    print('Image label dimensions:', labels.size())\n",
        "    print(labels[:10])\n",
        "    break\n",
        "\n",
        "# Dataset Sanity check 2:\n",
        "print('\\nValidation Set:')\n",
        "for images, labels in valid_loader:\n",
        "    print('Image batch dimensions:', images.size())\n",
        "    print('Image label dimensions:', labels.size())\n",
        "    print(labels[:10])\n",
        "    break\n",
        "\n",
        "# Dataset Sanity check 3:\n",
        "print('\\nTesting Set:')\n",
        "for images, labels in test_loader:\n",
        "    print('Image batch dimensions:', images.size())\n",
        "    print('Image label dimensions:', labels.size())\n",
        "    print(labels[:10])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "hGkhqNDRuNOi"
      },
      "outputs": [],
      "source": [
        "# Helper Class 1: Reshape is used to reshape the embeddings to the correct image size\n",
        "class Reshape(nn.Module):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "        self.shape = args\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.view(self.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "qgUkRT2huOny"
      },
      "outputs": [],
      "source": [
        "# Helper Class 2: Trim is used to remove the excess pixels at the output of the decoder block (28x28 as MNIST size)\n",
        "class Trim(nn.Module):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :28, :28]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "XEMXcPhNRd88"
      },
      "outputs": [],
      "source": [
        "class Conv_AutoEncoder(nn.Module):\n",
        "    def __init__(self, embedding_size):\n",
        "        super().__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "        # Encoder (input: 3x64x64)\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),      # 32x64x64\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # 64x32x32\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1), # 128x16x16\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),# 256x8x8\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 8 * 8, embedding_size)  # Adjust for input size\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(embedding_size, 256 * 8 * 8),\n",
        "            Reshape(-1, 256, 8, 8),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.Conv2d(32, 3, kernel_size=3, padding=1),\n",
        "            nn.Tanh()  # Use Tanh for [-1,1] normalization\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0kQ9teJumRT",
        "outputId": "8721dd88-6ece-4d52-97e9-715edd4490e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conv_AutoEncoder(\n",
            "  (encoder): Sequential(\n",
            "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.01)\n",
            "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (3): LeakyReLU(negative_slope=0.01)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): LeakyReLU(negative_slope=0.01)\n",
            "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (7): LeakyReLU(negative_slope=0.01)\n",
            "    (8): Flatten(start_dim=1, end_dim=-1)\n",
            "    (9): Linear(in_features=16384, out_features=2, bias=True)\n",
            "  )\n",
            "  (decoder): Sequential(\n",
            "    (0): Linear(in_features=2, out_features=16384, bias=True)\n",
            "    (1): Reshape()\n",
            "    (2): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (3): LeakyReLU(negative_slope=0.01)\n",
            "    (4): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (5): LeakyReLU(negative_slope=0.01)\n",
            "    (6): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (7): LeakyReLU(negative_slope=0.01)\n",
            "    (8): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): Tanh()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = Conv_AutoEncoder(embedding_size = 2)\n",
        "print(model)\n",
        "model.to(cuda_device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = lr_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "q3jKW4xWvS78"
      },
      "outputs": [],
      "source": [
        "def compute_epoch_loss_autoencoder(model, data_loader, loss_fn, device):\n",
        "    model.eval()\n",
        "    curr_loss, num_examples = 0., 0\n",
        "    with torch.no_grad():\n",
        "        for features, _ in data_loader:\n",
        "            features = features.to(device)\n",
        "            logits = model(features)\n",
        "            loss = loss_fn(logits, features, reduction = 'sum')\n",
        "            num_examples += features.size(0)\n",
        "            curr_loss += loss\n",
        "\n",
        "        curr_loss = curr_loss / num_examples\n",
        "        return curr_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "BlD9c78Ou8MK"
      },
      "outputs": [],
      "source": [
        "def train_autoencoder_v1(num_epochs, model, optimizer, device,\n",
        "                         train_loader, loss_fn = None,\n",
        "                         logging_interval = 100,\n",
        "                         skip_epoch_stats = False,\n",
        "                         save_model = None):\n",
        "\n",
        "    log_dict = {'train_loss_per_batch': [],\n",
        "                'train_loss_per_epoch': []}\n",
        "\n",
        "    if loss_fn is None:\n",
        "        loss_fn = F.mse_loss\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "        for batch_idx, (features, _) in enumerate(train_loader):\n",
        "\n",
        "            features = features.to(device)\n",
        "\n",
        "            # Forward and backpropagation ste[s]\n",
        "            logits = model(features)\n",
        "            loss = loss_fn(logits, features)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Model weights' update\n",
        "            optimizer.step()\n",
        "\n",
        "            # Performance logging\n",
        "            log_dict['train_loss_per_batch'].append(loss.item())\n",
        "\n",
        "            if not batch_idx % logging_interval:\n",
        "                print('Epoch: %03d/%03d | Batch %04d/%04d | Loss: %.4f'\n",
        "                      % (epoch+1, num_epochs, batch_idx,\n",
        "                          len(train_loader), loss))\n",
        "\n",
        "        if not skip_epoch_stats:\n",
        "            model.eval()\n",
        "\n",
        "            with torch.set_grad_enabled(False):  # Will help save memory during inference\n",
        "\n",
        "                train_loss = compute_epoch_loss_autoencoder(\n",
        "                    model, train_loader, loss_fn, device)\n",
        "                print('***Epoch: %03d/%03d | Loss: %.3f' % (\n",
        "                      epoch+1, num_epochs, train_loss))\n",
        "                log_dict['train_loss_per_epoch'].append(train_loss.item())\n",
        "\n",
        "        print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
        "\n",
        "    print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
        "    if save_model is not None:\n",
        "        torch.save(model.state_dict(), save_model)\n",
        "\n",
        "    return log_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1P3k0mPvbpD",
        "outputId": "721009fb-a8ab-4925-d649-3a61a9e3facc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/020 | Batch 0000/1108 | Loss: 0.3599\n"
          ]
        }
      ],
      "source": [
        "log_dict_2e = train_autoencoder_v1(num_epochs = num_epochs, model = model,\n",
        "                                optimizer = optimizer, device = cuda_device,\n",
        "                                train_loader = train_loader,\n",
        "                                skip_epoch_stats = True,\n",
        "                                logging_interval = 250)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0cbdIK1yzls"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), '/content/conv_ae_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XZiX0oNzJET"
      },
      "outputs": [],
      "source": [
        "torch.save(model, '/content/conv_ae_model_complete.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOH_sUSGwZ2a"
      },
      "outputs": [],
      "source": [
        "def plot_training_loss(minibatch_losses, num_epochs, averaging_iterations = 100, custom_label = ''):\n",
        "\n",
        "    iter_per_epoch = len(minibatch_losses) // num_epochs\n",
        "\n",
        "    plt.figure()\n",
        "    ax1 = plt.subplot(1, 1, 1)\n",
        "    ax1.plot(range(len(minibatch_losses)),\n",
        "             (minibatch_losses), label = f'Minibatch Loss{custom_label}')\n",
        "    ax1.set_xlabel('Iterations')\n",
        "    ax1.set_ylabel('Loss')\n",
        "\n",
        "    if len(minibatch_losses) < 1000:\n",
        "        num_losses = len(minibatch_losses) // 2\n",
        "    else:\n",
        "        num_losses = 1000\n",
        "\n",
        "    ax1.set_ylim([\n",
        "        0, np.max(minibatch_losses[num_losses:])*1.5\n",
        "        ])\n",
        "\n",
        "    ax1.plot(np.convolve(minibatch_losses,\n",
        "                         np.ones(averaging_iterations,)/averaging_iterations,\n",
        "                         mode = 'valid'),\n",
        "             label = f'Running Average{custom_label}')\n",
        "    ax1.legend()\n",
        "\n",
        "    ###################\n",
        "    # Set scond x-axis\n",
        "    ax2 = ax1.twiny()\n",
        "    newlabel = list(range(num_epochs+1))\n",
        "\n",
        "    newpos = [e*iter_per_epoch for e in newlabel]\n",
        "\n",
        "    ax2.set_xticks(newpos[::10])\n",
        "    ax2.set_xticklabels(newlabel[::10])\n",
        "\n",
        "    ax2.xaxis.set_ticks_position('bottom')\n",
        "    ax2.xaxis.set_label_position('bottom')\n",
        "    ax2.spines['bottom'].set_position(('outward', 45))\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_xlim(ax1.get_xlim())\n",
        "    ###################\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvV2Uyw4wY76"
      },
      "outputs": [],
      "source": [
        "plot_training_loss(log_dict_2e['train_loss_per_batch'], num_epochs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aodYyAERw6FT"
      },
      "outputs": [],
      "source": [
        "def plot_generated_images(data_loader, model, device,\n",
        "                          unnormalizer = None,\n",
        "                          figsize = (20, 2.5), n_images = 15, modeltype = 'autoencoder'):\n",
        "\n",
        "    fig, axes = plt.subplots(nrows = 2, ncols = n_images,\n",
        "                             sharex = True, sharey = True, figsize = figsize)\n",
        "\n",
        "    for batch_idx, (features, _) in enumerate(data_loader):\n",
        "\n",
        "        features = features.to(device)\n",
        "\n",
        "        color_channels = features.shape[1]\n",
        "        image_height = features.shape[2]\n",
        "        image_width = features.shape[3]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if modeltype == 'autoencoder':\n",
        "                decoded_images = model(features)[:n_images]\n",
        "            elif modeltype == 'VAE':\n",
        "                encoded, z_mean, z_log_var, decoded_images = model(features)[:n_images]\n",
        "            else:\n",
        "                raise ValueError('`modeltype` not supported')\n",
        "\n",
        "        orig_images = features[:n_images]\n",
        "        break\n",
        "\n",
        "    for i in range(n_images):\n",
        "        for ax, img in zip(axes, [orig_images, decoded_images]):\n",
        "            curr_img = img[i].detach().to(torch.device('cpu'))\n",
        "            unnormalize = transforms.Normalize(mean=[-1, -1, -1], std=[2, 2, 2])\n",
        "            if unnormalizer is not None:\n",
        "                curr_img = unnormalizer(curr_img)\n",
        "\n",
        "            if color_channels > 1:\n",
        "                curr_img = np.transpose(curr_img, (1, 2, 0))\n",
        "                ax[i].imshow(curr_img)\n",
        "            else:\n",
        "                ax[i].imshow(curr_img.view((image_height, image_width)), cmap = 'binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF5MSAGgwySC"
      },
      "outputs": [],
      "source": [
        "plot_generated_images(data_loader = train_loader, model = model, device = cuda_device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "from torchvision import datasets\n",
        "\n",
        "def plot_latent_space(model, test_loader, dataset, device, attribute_idx=31, n_samples=1000):\n",
        "    embeddings = []\n",
        "    labels = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, image_indices) in enumerate(test_loader):\n",
        "            images = images.to(device)\n",
        "            embeddings.append(model.encoder(images).cpu().numpy())\n",
        "\n",
        "            # attributes from the dataset using image indices\n",
        "            batch_labels = [dataset.targets[idx] for idx in image_indices]\n",
        "            labels.append(np.array(batch_labels))\n",
        "\n",
        "            if batch_idx * test_loader.batch_size >= n_samples:\n",
        "                break\n",
        "\n",
        "    embeddings = np.concatenate(embeddings)\n",
        "    labels = np.concatenate(labels)\n",
        "\n",
        "    # Random subset for faster computation\n",
        "    rand_idx = np.random.choice(len(embeddings), size=min(n_samples, len(embeddings)), replace=False)\n",
        "    embeddings = embeddings[rand_idx]\n",
        "    labels = labels[rand_idx]\n",
        "\n",
        "    # PCA Projection\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_results = pca.fit_transform(embeddings)\n",
        "\n",
        "    # t-SNE Projection\n",
        "    tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
        "    tsne_results = tsne.fit_transform(embeddings)\n",
        "\n",
        "    plt.figure(figsize=(16, 6))\n",
        "\n",
        "    # PCA Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    scatter = plt.scatter(pca_results[:, 0], pca_results[:, 1], c=labels,\n",
        "                         cmap='viridis', alpha=0.6, s=10)\n",
        "    plt.colorbar(scatter, label='Smiling Probability')\n",
        "    plt.title('PCA Projection of Latent Space')\n",
        "    plt.xlabel('PCA Component 1')\n",
        "    plt.ylabel('PCA Component 2')\n",
        "\n",
        "    # t-SNE Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    scatter = plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=labels,\n",
        "                         cmap='viridis', alpha=0.6, s=10)\n",
        "    plt.colorbar(scatter, label='Smiling Probability')\n",
        "    plt.title('t-SNE Projection of Latent Space')\n",
        "    plt.xlabel('t-SNE Component 1')\n",
        "    plt.ylabel('t-SNE Component 2')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "hcuQotX8jPP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_latent_space(model, test_loader, dataset, device=cuda_device, n_samples=2000)"
      ],
      "metadata": {
        "id": "pi7w5CyPlULJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n0qcDN0zVEb"
      },
      "outputs": [],
      "source": [
        "model = Conv_AutoEncoder(embedding_size=2)  # Initialize the model\n",
        "# Option 1: Loading model params\n",
        "model.load_state_dict(torch.load('/content/conv_ae_model.pth'))\n",
        "model.to(cuda_device) # This line is added to move the model to the GPU\n",
        "model.eval()  # Set to evaluation mode\n",
        "\n",
        "# Option 2: Loading full model\n",
        "# model = torch.load('/content/conv_ae_model_complete.pth')\n",
        "# model.eval()  # Set to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeBwjVpOsiWn"
      },
      "outputs": [],
      "source": [
        "all_embeddings = torch.zeros((10_000, 2))\n",
        "\n",
        "num_images = 0\n",
        "for images, labels in train_loader:\n",
        "\n",
        "    if num_images >= 10_000:\n",
        "        break\n",
        "\n",
        "    begin = num_images\n",
        "    end = begin + images.size(0)\n",
        "    end = min(end, 10_000)\n",
        "\n",
        "\n",
        "    images, labels = images.to(cuda_device), labels.to(cuda_device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        embeddings = model.encoder(images).to('cpu')\n",
        "\n",
        "    # Adjust the slice size to match the available space:\n",
        "    all_embeddings[begin:end] = embeddings[:end-begin]\n",
        "\n",
        "\n",
        "    num_images = end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQ0jkBdUtMNM"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(nrows = 1, ncols = 2,\n",
        "                         sharex = True, sharey = True,\n",
        "                         figsize = (6, 4))\n",
        "\n",
        "axes[0].hist(all_embeddings[:, 0].numpy())\n",
        "axes[0].set_title('Histogram of Dim-1 Latent Vector', fontsize = 8)\n",
        "axes[1].hist(all_embeddings[:, 1].numpy())\n",
        "axes[1].set_title('Histogram of Dim-2 Latent Vector', fontsize = 8)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4J4Ne2jVEQN"
      },
      "outputs": [],
      "source": [
        "def extract_embeddings(model, dataloader, device, max_images=10_000):\n",
        "    model.eval()\n",
        "    embeddings_dim = model.encoder(next(iter(dataloader))[0].to(device)).shape[1]\n",
        "    all_embeddings = torch.zeros((max_images, embeddings_dim))\n",
        "\n",
        "    num_images = 0\n",
        "    with torch.no_grad():\n",
        "        for images, _ in dataloader:\n",
        "            if num_images >= max_images:\n",
        "                break\n",
        "\n",
        "            batch_size = images.size(0)\n",
        "            begin = num_images\n",
        "            end = min(begin + batch_size, max_images)\n",
        "\n",
        "            images = images.to(device)\n",
        "            embeddings = model.encoder(images).to('cpu')\n",
        "\n",
        "            all_embeddings[begin:end] = embeddings[:end-begin]\n",
        "            num_images = end\n",
        "\n",
        "    return all_embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IOoDz2_Tcu4"
      },
      "outputs": [],
      "source": [
        "def decode_and_plot(model, latent_vector, device='cuda', cmap='binary'):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Ensure latent vector is a tensor and has batch dimension\n",
        "        latent_tensor = torch.tensor(latent_vector, dtype=torch.float32).to(device)\n",
        "        if latent_tensor.ndim == 1:\n",
        "            latent_tensor = latent_tensor.unsqueeze(0)\n",
        "\n",
        "        decoded_image = model.decoder(latent_tensor)\n",
        "        decoded_image = decoded_image.squeeze()  # remove all dimensions of size 1\n",
        "\n",
        "        # Move to CPU and transpose before plotting\n",
        "        decoded_image = decoded_image.cpu().numpy()\n",
        "        decoded_image = decoded_image.transpose(1, 2, 0)  # Transpose to (height, width, channels)\n",
        "\n",
        "\n",
        "        plt.imshow(decoded_image, cmap=cmap)\n",
        "        plt.axis('off')\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ylBeBqbYJ8_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "def create_embedding_animation(model, test_loader, device):\n",
        "    model.eval()\n",
        "\n",
        "    # Get first test batch\n",
        "    test_batch = next(iter(test_loader))[0].to(device)\n",
        "\n",
        "    # Create figure\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "    # Get original embedding\n",
        "    with torch.no_grad():\n",
        "        orig_embedding = model.encoder(test_batch[0:1])\n",
        "        orig_img = model.decoder(orig_embedding).cpu()\n",
        "\n",
        "    # Initialize plots\n",
        "    # Permute the dimensions of the image tensor to (height, width, channels)\n",
        "    im1 = ax1.imshow(test_batch[0].cpu().permute(1, 2, 0), cmap='gray')\n",
        "    im2 = ax2.imshow(orig_img.squeeze().permute(1, 2, 0), cmap='gray')\n",
        "    ax1.set_title('Original Image')\n",
        "    ax2.set_title('Reconstructed Image')\n",
        "    plt.close()\n",
        "\n",
        "    def update(frame):\n",
        "        # Modify first embedding dimension\n",
        "        modified_embedding = orig_embedding.clone()\n",
        "        modified_embedding[0,0] = frame  # Vary first dimension\n",
        "\n",
        "        # Reconstruct\n",
        "        with torch.no_grad():\n",
        "            recon_img = model.decoder(modified_embedding).cpu()\n",
        "\n",
        "        # Update images\n",
        "        im1.set_data(test_batch[0].cpu().permute(1, 2, 0))\n",
        "        im2.set_data(recon_img.squeeze().permute(1, 2, 0))\n",
        "        ax2.set_title(f'Reconstructed (z1={frame:.1f})')\n",
        "        return im1, im2\n",
        "\n",
        "    # Create animation\n",
        "    ani = animation.FuncAnimation(\n",
        "        fig, update,\n",
        "        frames=np.arange(-10, 10, 0.1),\n",
        "        interval=50\n",
        "    )\n",
        "\n",
        "    return ani"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract embeddings\n",
        "embeddings = extract_embeddings(model, train_loader, device=cuda_device)\n",
        "\n",
        "# Decode a specific embedding\n",
        "decode_and_plot(model, embeddings[0], device=cuda_device)\n"
      ],
      "metadata": {
        "id": "9sdvdGNEhQqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "animation_2e = create_embedding_animation(model, test_loader, cuda_device)\n",
        "HTML(animation_2e.to_jshtml())"
      ],
      "metadata": {
        "id": "nlKvLrochXRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "animation_2e.save('CelebA_embedding_animation_2e.gif', writer='imagemagick')"
      ],
      "metadata": {
        "id": "r7LzxVoqhhc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atyKIDevx4ek"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gprAAEdQP8jr"
      },
      "source": [
        "# Training with 32 Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-Q65uqb_7JBb"
      },
      "outputs": [],
      "source": [
        "model_32e = Conv_AutoEncoder(embedding_size=32).to(cuda_device)\n",
        "optimizer_32e = torch.optim.Adam(model_32e.parameters(), lr=lr_rate)\n",
        "\n",
        "    # Train model\n",
        "log_dict_32e = train_autoencoder_v1(\n",
        "        num_epochs=num_epochs,\n",
        "        model=model_32e,\n",
        "        optimizer=optimizer_32e,\n",
        "        device=cuda_device,\n",
        "        train_loader=train_loader,\n",
        "        skip_epoch_stats=True,\n",
        "        logging_interval=250\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_loss(log_dict_32e['train_loss_per_batch'], num_epochs)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KEnO1kDVl48t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQFDo3eTRGhj"
      },
      "outputs": [],
      "source": [
        "plot_latent_space(model_32e, test_loader, dataset, device=cuda_device, n_samples=2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt3pEhg_Qojh"
      },
      "outputs": [],
      "source": [
        "plot_generated_images(data_loader = train_loader, model = model_32e, device = cuda_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cGbUwkKRa8f"
      },
      "outputs": [],
      "source": [
        "# Extract embeddings\n",
        "embeddings = extract_embeddings(model_32e, train_loader, device=cuda_device)\n",
        "\n",
        "# Decode a specific embedding\n",
        "decode_and_plot(model_32e, embeddings[0], device=cuda_device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrQfxRefYOYy"
      },
      "outputs": [],
      "source": [
        "animation_32e = create_embedding_animation(model_32e, test_loader, cuda_device)\n",
        "HTML(animation_32e.to_jshtml())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIAwztmfZyfj"
      },
      "outputs": [],
      "source": [
        "animation_32e.save('CelebA_embedding_animation_32e.gif', writer='imagemagick')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9WTX2X1P6Cw"
      },
      "source": [
        "# Training with 64 Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jCS-cvp97qT7"
      },
      "outputs": [],
      "source": [
        "model_64e = Conv_AutoEncoder(embedding_size=64).to(cuda_device)\n",
        "optimizer_64e = torch.optim.Adam(model_64e.parameters(), lr=lr_rate)\n",
        "\n",
        "    # Train model\n",
        "log_dict_64e = train_autoencoder_v1(\n",
        "        num_epochs=num_epochs,\n",
        "        model=model_64e,\n",
        "        optimizer=optimizer_64e,\n",
        "        device=cuda_device,\n",
        "        train_loader=train_loader,\n",
        "        skip_epoch_stats=True,\n",
        "        logging_interval=250\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68Q7gkyF7spz"
      },
      "outputs": [],
      "source": [
        "plot_training_loss(log_dict_64e['train_loss_per_batch'], num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_latent_space(model_64e, test_loader, dataset, device=cuda_device, n_samples=2000)"
      ],
      "metadata": {
        "id": "nqNhAwd-mJSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o88jG_IjDXQ"
      },
      "outputs": [],
      "source": [
        "plot_generated_images(data_loader = train_loader, model = model_64e, device = cuda_device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkHkZ8wkjJlj"
      },
      "outputs": [],
      "source": [
        "# Extract embeddings\n",
        "embeddings = extract_embeddings(model_64e, train_loader, device=cuda_device)\n",
        "\n",
        "# Decode a specific embedding\n",
        "decode_and_plot(model_64e, embeddings[0], device=cuda_device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgvvrSKmjMlO"
      },
      "outputs": [],
      "source": [
        "animation_64e = create_embedding_animation(model_64e, test_loader, cuda_device)\n",
        "HTML(animation_64e.to_jshtml())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkHhCe-mjO4D"
      },
      "outputs": [],
      "source": [
        "animation_64e.save('CelebA_embedding_animation_64e.gif', writer='imagemagick')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}